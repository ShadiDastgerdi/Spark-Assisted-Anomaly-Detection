A Kafka source is created for a streaming query. We specify the address of kafka.bootstrap.servers, the desired topic, and set startingOffsets to the earliest so that it starts consuming from the first offset. At this stage, check_anomaly function is called. First, we parse the received value from the data source to the desired data schema. Due to the fact that the data is structured and separated by commas, it can be easily read by Spark functions, but here to show the UDF capability in Spark, a function named parse_data_from_kafka_message creates the desired dataframe based on the defined schema. Then we convert the time column, which is defined as long, to timestamp. In this step, we create one-second windows of data and then calculate the average cpu in this interval. We check the average obtained in each window to determine its status according to 2 predetermined parameters, average and standard deviation. To select the mentioned 2 parameters, I calculated the average and standard deviation by reading one third of the data (3000 records). The desired status of each window is stored in a new column called status. Finally, warnings and errors are displayed in the console.
